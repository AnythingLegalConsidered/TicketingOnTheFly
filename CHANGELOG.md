# Journal de Bord - Syst√®me de Ticketing Int√©gr√©

Ce fichier documente toutes les actions, commandes et modifications effectu√©es sur le projet TicketingOnTheFly.

---

## 2025-10-22 - Initialisation du Projet

### Actions Pr√©liminaires

#### Nettoyage et R√©organisation du D√©p√¥t Git
**Objectif :** Remplacer l'ancien contenu du d√©p√¥t par le travail local actuel

```bash
# Ajout de tous les changements locaux
git add -A

# Commit du nouveau contenu
git commit -m "Replace entire project with local work"

# Push forc√© pour remplacer le contenu distant
git push origin main --force
```

**R√©sultat :** Le d√©p√¥t distant contient maintenant uniquement la structure locale simplifi√©e.

---

#### Ajout du Dossier de Documentation
**Objectif :** Ajouter le dossier `Doc` contenant toute la documentation du projet

```bash
# Ajout du dossier Doc au staging
git add -A Doc

# Commit
git commit -m "Add Doc directory"

# Push
git push origin main
```

**Fichiers ajout√©s :**
- 10 fichiers Markdown de documentation (00 √† 09)
- 1 image (Pasted image 20251021213622.png)

---

#### Configuration des Fins de Ligne
**Objectif :** Normaliser les fins de ligne (LF) et √©viter les warnings Git sur Windows

**Fichier cr√©√© :** `.gitattributes`

```gitattributes
# Normalize line endings
*.md text eol=lf
*.yml text eol=lf
*.yaml text eol=lf
*.sh text eol=lf
*.py text eol=lf
# Binary files
*.png binary
*.jpg binary
*.jpeg binary
*.gif binary
```

```bash
# Ajout et commit
git add .gitattributes
git commit -m "Add .gitattributes to normalize EOL"
git push origin main
```

---

#### Renommage Doc ‚Üí doc
**Objectif :** Uniformiser la casse (minuscule) pour le dossier de documentation

```bash
# √âtape 1 : Renommage temporaire (n√©cessaire sur syst√®mes case-insensitive)
git mv Doc Doc_tmp
git commit -m "Temp rename Doc -> Doc_tmp to change case"

# √âtape 2 : Renommage final
git mv Doc_tmp doc
git commit -m "Rename Doc to doc (lowercase)"

# Push
git push origin main
```

**R√©sultat :** Le dossier est maintenant `doc/` dans le d√©p√¥t.

---

#### Cr√©ation de la Branche de Sauvegarde
**Objectif :** Cr√©er un point de restauration avant l'ajout du dossier doc

```bash
# Cr√©ation de la branche pointant sur le commit pr√©c√©dent
git branch backup-before-doc fc8174b

# Push de la branche
git push origin backup-before-doc
```

**Branches disponibles :**
- `main` : branche principale de d√©veloppement
- `backup-before-doc` : sauvegarde de l'√©tat initial (commit fc8174b)

---

## 2025-10-22 - Phase 1-3 : Infrastructure de Base

### Correction des Liens Portainer
**Objectif :** R√©soudre le probl√®me des liens Portainer pointant vers 0.0.0.0 (invalide)

**Probl√®me identifi√© :**
- Portainer g√©n√®re des liens comme `http://0.0.0.0:8081` qui ne fonctionnent pas
- Les ports Docker √©taient expos√©s sur toutes les interfaces (0.0.0.0)

**Solution :** Lier explicitement les ports √† `127.0.0.1` (localhost)

**Fichier modifi√© :** `docker-compose.yml`

```yaml
# Avant :
ports:
  - "8081:8080"

# Apr√®s :
ports:
  - "127.0.0.1:8081:8080"
```

**Ports modifi√©s :**
- Zammad nginx: `127.0.0.1:8081:8080`
- Zammad railsserver: `127.0.0.1:8082:3000`
- phpLDAPadmin: `127.0.0.1:8080:80`
- Portainer: `127.0.0.1:9000:9000` et `127.0.0.1:9443:9443`

```bash
# Arr√™t des conteneurs
docker-compose down

# Red√©marrage avec nouvelle configuration
docker-compose up -d

# V√©rification
docker ps --format "table {{.Names}}\t{{.Ports}}"

# Commit et push
git add docker-compose.yml
git commit -m "Fix Portainer links: bind ports to 127.0.0.1 instead of 0.0.0.0"
git push origin main
```

**R√©sultat :** 
- Les liens Portainer fonctionnent maintenant correctement
- Am√©lioration de la s√©curit√© (services accessibles uniquement depuis localhost)

---

## 2025-10-22 - Phase 4 : Inventaire et Gestion du Parc (OCS Inventory)

### Objectif de cette Phase
D√©ployer OCS Inventory pour l'inventaire automatis√© du parc informatique, avec pr√©paration pour l'int√©gration future avec Zammad.

### √âtapes R√©alis√©es

#### 1. Ajout des Variables d'Environnement
**Fichier modifi√© :** `.env`

```bash
# Ajout des variables pour OCS Inventory
OCS_DB_NAME=ocsdb
OCS_DB_USER=ocs
OCS_DB_PASSWORD=SecureOCSPassword789
MARIADB_ROOT_PASSWORD=VeryStrongMariaDBRootPass321
```

**Note de s√©curit√© :** Ces mots de passe sont des exemples. En production, utilisez des mots de passe forts g√©n√©r√©s al√©atoirement.

---

#### 2. Ajout des Services OCS au docker-compose.yml
**Fichier modifi√© :** `docker-compose.yml`

**Services ajout√©s :**

1. **ocs-db** (Base de donn√©es MariaDB)
   - Image: `mariadb:10.11`
   - Port interne: 3306
   - Volume: `ocs_db_data`
   
2. **ocs-server** (Serveur OCS Inventory)
   - Image: `ocsinventory/ocsinventory-docker-image:latest`
   - Port expos√©: `127.0.0.1:8083:80`
   - Volumes:
     - `ocs_data` : donn√©es des rapports
     - `ocs_perlcomdata` : configuration Perl
     - `ocs_ocsreportsdata` : donn√©es des rapports OCS

**Variables d'environnement OCS :**
```yaml
environment:
  - OCS_DBSERVER_READ=ocs-db
  - OCS_DBSERVER_WRITE=ocs-db
  - OCS_DBNAME=${OCS_DB_NAME}
  - OCS_DBUSER=${OCS_DB_USER}
  - OCS_DBPASS=${OCS_DB_PASSWORD}
  - TZ=${TZ}
```

---

#### 3. R√©solution des Probl√®mes de D√©marrage

**Probl√®me rencontr√© :** L'image `ocsinventory/ocsinventory-docker-image:2.12.1` causait des erreurs de configuration Apache/Perl.

**Solution appliqu√©e :**
```bash
# Tentatives avec diff√©rentes versions
# Version 2.12.1 -> Erreur de configuration Perl
# Version 2.12.0 -> Image non trouv√©e
# Version latest -> ‚úÖ Fonctionne correctement
```

**Commandes de d√©pannage :**
```bash
# V√©rifier les logs du conteneur
docker logs ocs-server --tail 30

# Red√©marrer le service avec la nouvelle configuration
docker-compose down ocs-server
docker-compose up -d ocs-server

# V√©rifier l'√©tat final
docker-compose ps
```

---

#### 4. V√©rification du D√©ploiement

```bash
# V√©rifier que les conteneurs sont d√©marr√©s
docker-compose ps | Select-String -Pattern "ocs-"
```

**R√©sultat :**
```
ocs-db      mariadb:10.11                          Up      3306/tcp
ocs-server  ocsinventory/..:latest                 Up      127.0.0.1:8083->80/tcp
```

‚úÖ **Les deux conteneurs OCS sont maintenant op√©rationnels.**

---

### Configuration Post-Installation OCS Inventory

#### Acc√®s √† l'Interface Web
- **URL:** `http://localhost:8083/ocsreports`
- **Port:** 8083 (li√© √† 127.0.0.1 pour la s√©curit√©)

#### √âtapes de Configuration Initiale

1. **Premier acc√®s - Assistant d'installation**
   ```
   Ouvrir : http://localhost:8083/ocsreports
   ```
   
2. **Informations de connexion √† la base de donn√©es**
   - MySQL Server: `ocs-db`
   - MySQL User: `ocs` (valeur de OCS_DB_USER)
   - MySQL Password: La valeur d√©finie dans OCS_DB_PASSWORD
   - Database Name: `ocsdb` (valeur de OCS_DB_NAME)
   
3. **Lancer l'installation**
   - Cliquer sur "Send"
   - L'installeur cr√©e la structure de la base de donn√©es
   
4. **Connexion initiale**
   - Utilisateur par d√©faut: `admin`
   - Mot de passe par d√©faut: `admin`

#### ‚ö†Ô∏è √âTAPES DE S√âCURIT√â CRITIQUES

**1. Supprimer le fichier d'installation**
```bash
# Commande √† ex√©cuter IMM√âDIATEMENT apr√®s l'installation
docker-compose exec ocs-server rm /usr/share/ocsinventory-reports/ocsreports/install.php
```

**2. Changer le mot de passe administrateur**
- Se connecter avec `admin` / `admin`
- Menu utilisateur (ic√¥ne en haut √† droite) > "User profile"
- Changer imm√©diatement le mot de passe

**3. V√©rifier la suppression**
```bash
# V√©rifier que install.php n'existe plus
docker-compose exec ocs-server ls -la /usr/share/ocsinventory-reports/ocsreports/ | grep install
```

---

### Prochaines √âtapes

1. **D√©ploiement des Agents OCS**
   - Installer les agents OCS sur les postes clients Windows/Linux/Mac
   - Configurer les agents pour pointer vers `http://[IP_SERVEUR]:8083/ocsinventory`

2. **Int√©gration avec Zammad**
   - Configurer le plugin d'int√©gration OCS dans Zammad
   - Associer les tickets aux machines de l'inventaire

3. **Configuration Avanc√©e**
   - D√©finir les groupes de machines
   - Configurer les rapports automatiques
   - Mettre en place les r√®gles d'inventaire

---

## 2025-10-22 - Phase 5 : Documentation Interne avec Wiki.js

### Objectif de cette Phase
D√©ployer Wiki.js comme plateforme de documentation interne pour l'√©quipe technique, distincte de la base de connaissances Zammad (orient√©e utilisateurs finaux).

### Distinction Documentation : Wiki.js vs Zammad
- **Base de connaissances Zammad** : Orient√©e client/utilisateur final, solutions simples aux probl√®mes courants
- **Wiki.js** : Orient√©e √©quipe technique, documentation d'infrastructure, proc√©dures avanc√©es, guides techniques

### √âtapes R√©alis√©es

#### 1. Ajout de la Variable d'Environnement
**Fichier modifi√© :** `.env`

```bash
# Ajout de la variable pour la base de donn√©es Wiki.js
WIKIJS_DB_NAME=wikijs
```

**Note :** Wiki.js partage le serveur PostgreSQL de Zammad mais utilise sa propre base de donn√©es pour une s√©paration logique des donn√©es.

---

#### 2. Ajout du Service Wiki.js au docker-compose.yml
**Fichier modifi√© :** `docker-compose.yml`

**Service ajout√© :**

```yaml
wikijs:
  image: requarks/wiki:2
  container_name: wikijs
  restart: unless-stopped
  depends_on:
    - zammad-db
  environment:
    - DB_TYPE=postgres
    - DB_HOST=zammad-db
    - DB_PORT=5432
    - DB_USER=${POSTGRES_USER}
    - DB_PASS=${POSTGRES_PASSWORD}
    - DB_NAME=${WIKIJS_DB_NAME}
    - TZ=${TZ}
  ports:
    - "127.0.0.1:8084:3000"
  volumes:
    - wikijs_data:/wiki/data
  networks:
    - ticketing_network
```

**Volume ajout√© :** `wikijs_data`

**‚ö†Ô∏è Changement de Port :** 
- Document original: Port 8083
- **Port utilis√©: 8084** (8083 d√©j√† utilis√© par OCS Inventory)

---

#### 3. Cr√©ation Manuelle de la Base de Donn√©es

**Probl√®me rencontr√© :** Wiki.js ne cr√©e pas automatiquement sa base de donn√©es dans PostgreSQL.

**Solution :**
```bash
# Cr√©er la base de donn√©es wikijs dans PostgreSQL
docker-compose exec zammad-db psql -U admin -d postgres -c "CREATE DATABASE wikijs;"

# Red√©marrer Wiki.js pour qu'il se connecte
docker-compose restart wikijs
```

**R√©sultat :** La connexion √† la base de donn√©es a r√©ussi et le serveur HTTP d√©marre correctement.

---

#### 4. D√©marrage et V√©rification

```bash
# Lancer Wiki.js
docker-compose up -d wikijs

# V√©rifier l'√©tat
docker-compose ps | Select-String -Pattern "wikijs"

# V√©rifier les logs
docker logs wikijs --tail 30
```

**Logs de succ√®s :**
```
2025-10-22T11:36:25.767Z [MASTER] info: Database Connection Successful [ OK ]
2025-10-22T11:36:25.860Z [MASTER] info: HTTP Server on port: [ 3000 ]
2025-10-22T11:36:25.862Z [MASTER] info: HTTP Server: [ RUNNING ]
2025-10-22T11:36:25.863Z [MASTER] info: Browse to http://YOUR-SERVER-IP:3000/ to complete setup!
```

‚úÖ **Wiki.js est maintenant op√©rationnel et pr√™t pour la configuration initiale.**

---

### Configuration Post-Installation Wiki.js

#### Acc√®s √† l'Interface Web
- **URL:** `http://localhost:8084`
- **Port:** 8084 (li√© √† 127.0.0.1 pour la s√©curit√©)

#### √âtapes de Configuration Initiale

1. **Premier acc√®s - Assistant d'installation**
   ```
   Ouvrir : http://localhost:8084
   ```

2. **Cr√©ation du compte administrateur**
   - Adresse email (sera l'identifiant de connexion)
   - Mot de passe fort (minimum recommand√©: 12 caract√®res)

3. **Configuration de l'URL du site**
   - URL de d√©veloppement: `http://localhost:8084`
   - URL de production (avec Traefik): `https://wiki.mondomaine.com`

4. **Finaliser l'installation**
   - Cliquer sur "Install"
   - Se connecter avec les identifiants cr√©√©s

---

### Configuration de l'Authentification LDAP

**Objectif :** Permettre aux utilisateurs de l'annuaire OpenLDAP de se connecter √† Wiki.js

#### Proc√©dure dans l'Interface Wiki.js

1. **Acc√©der √† l'administration**
   - Menu en haut √† droite > "Administration"

2. **Configurer LDAP**
   - Menu gauche > "Authentification"
   - Cliquer sur "LDAP / Active Directory"
   - Activer la strat√©gie (slider)

3. **Param√®tres de connexion LDAP**
   ```
   Host: openldap
   Port: 389
   Bind DN: cn=admin,dc=localhost
   Password: [Valeur de LDAP_ADMIN_PASSWORD du .env]
   Base DN: ou=users,dc=localhost
   User Login Field: uid
   ```

4. **Profile Mapping (Correspondance des champs)**
   ```
   Username: uid
   Display Name: cn
   Email: mail
   ```

5. **Sauvegarder**
   - Cliquer sur "Appliquer" en haut √† droite

6. **Tester**
   - Se d√©connecter
   - Se reconnecter avec un utilisateur LDAP

---

### R√©capitulatif des Ports (Mise √† jour)

| Service          | Port Local      | URL                            |
|------------------|-----------------|--------------------------------|
| phpLDAPadmin     | 8080            | http://localhost:8080          |
| Zammad           | 8081            | http://localhost:8081          |
| Zammad Rails     | 8082            | http://localhost:8082          |
| OCS Inventory    | 8083            | http://localhost:8083/ocsreports |
| **Wiki.js**      | **8084**        | **http://localhost:8084**      |
| Portainer (HTTP) | 9000            | http://localhost:9000          |
| Portainer (HTTPS)| 9443            | https://localhost:9443         |

---

### Prochaines √âtapes

1. **Configuration Post-Installation**
   - Cr√©er le compte administrateur
   - Configurer l'authentification LDAP
   - Cr√©er la structure initiale du wiki

2. **Phase 6 : Supervision**
   - D√©ployer Prometheus pour la collecte de m√©triques
   - D√©ployer Grafana pour la visualisation
   - Configurer les alertes

3. **Phase 7 : Reverse Proxy**
   - Configurer Traefik pour l'acc√®s unifi√©
   - Mettre en place les certificats SSL/TLS automatiques
   - Configurer les noms de domaine

---

## 2025-10-22 - Phase 6 : Supervision et Monitoring avec Prometheus & Grafana

### Objectif de cette Phase
D√©ployer une pile de supervision compl√®te pour surveiller la sant√© et les performances de tous les conteneurs Docker en temps r√©el. Passer d'un mode **r√©actif** √† un mode **proactif**.

### Th√©orie : Pourquoi la Supervision est Critique

#### Surveillance Proactive vs R√©active
- **Mode r√©actif** (AVANT) : "Un utilisateur appelle car le site est lent"
- **Mode proactif** (APR√àS) : "Je vois que le CPU du serveur de BDD augmente dangereusement, j'investigue avant impact"

#### Indicateurs Cl√©s Surveill√©s
- Sant√© des conteneurs (d√©marr√©s, arr√™t√©s, red√©marrages en boucle)
- Consommation ressources (CPU, RAM, I/O disque, r√©seau)
- Performances applicatives (temps de r√©ponse, nombre de requ√™tes)

#### Architecture de Supervision

**Le Duo Prometheus & Grafana :**
1. **Prometheus** : Base de donn√©es optimis√©e pour s√©ries temporelles
   - Fonctionne en "scraping" : interroge les cibles toutes les 15s sur `/metrics`
   - Stocke les m√©triques avec timestamp

2. **Grafana** : Interface de visualisation
   - Ne collecte pas de donn√©es
   - Se connecte √† Prometheus
   - Cr√©e des tableaux de bord interactifs

3. **cAdvisor** : Exportateur de m√©triques Docker
   - Expose automatiquement les m√©triques de TOUS les conteneurs
   - Prometheus scrape cAdvisor pour avoir vue compl√®te

---

### √âtapes R√©alis√©es

#### 1. Cr√©ation du Fichier de Configuration Prometheus

**Fichier cr√©√© :** `config/prometheus/prometheus.yml`

```yaml
# Fichier de configuration global de Prometheus
global:
  scrape_interval: 15s # Interroge les cibles toutes les 15 secondes
  evaluation_interval: 15s # √âvalue les r√®gles toutes les 15 secondes

# Liste des jobs de scraping
scrape_configs:
  # Job pour surveiller Prometheus lui-m√™me
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  # Job pour surveiller les m√©triques des conteneurs via cAdvisor
  - job_name: 'cadvisor'
    static_configs:
      - targets: ['cadvisor:8080']
```

**Explication :**
- **Job 1 (prometheus)** : Prometheus se surveille lui-m√™me
- **Job 2 (cadvisor)** : Prometheus r√©cup√®re les m√©triques de tous les conteneurs via cAdvisor

---

#### 2. Ajout des Services au docker-compose.yml

**Fichier modifi√© :** `docker-compose.yml`

**Services ajout√©s :**

1. **Prometheus** (Collecteur de m√©triques)
```yaml
prometheus:
  image: prom/prometheus:v2.47.1
  container_name: prometheus
  restart: unless-stopped
  ports:
    - "127.0.0.1:9090:9090"
  volumes:
    - ./config/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
    - prometheus_data:/prometheus
  command:
    - '--config.file=/etc/prometheus/prometheus.yml'
    - '--storage.tsdb.path=/prometheus'
  networks:
    - ticketing_network
```

2. **Grafana** (Visualisation)
```yaml
grafana:
  image: grafana/grafana-oss:10.1.5
  container_name: grafana
  restart: unless-stopped
  environment:
    - TZ=${TZ}
  ports:
    - "127.0.0.1:8085:3000"
  volumes:
    - grafana_data:/var/lib/grafana
  networks:
    - ticketing_network
```

3. **cAdvisor** (Exportateur m√©triques Docker)
```yaml
cadvisor:
  image: gcr.io/cadvisor/cadvisor:v0.47.2
  container_name: cadvisor
  restart: unless-stopped
  privileged: true
  devices:
    - /dev/kmsg
  volumes:
    - /:/rootfs:ro
    - /var/run:/var/run:ro
    - /sys:/sys:ro
    - /var/lib/docker/:/var/lib/docker:ro
    - /dev/disk/:/dev/disk:ro
  networks:
    - ticketing_network
```

**Volumes ajout√©s :**
- `prometheus_data` : Stockage des m√©triques
- `grafana_data` : Donn√©es Grafana

**‚ö†Ô∏è Gestion des Conflits de Ports :**
- Document original sugg√®re port 8084 pour Grafana
- **Port utilis√©: 8085** (8084 d√©j√† pris par Wiki.js, 3000 interne de Grafana)

---

#### 3. D√©marrage et V√©rification

```bash
# Lancer tous les services
docker-compose up -d

# V√©rifier l'√©tat des services de supervision
docker-compose ps | Select-String -Pattern "prometheus|grafana|cadvisor"

# V√©rifier les logs Prometheus
docker logs prometheus --tail 20
```

**R√©sultat :**
```
cadvisor     Up 34 seconds (healthy)   8080/tcp
grafana      Up 34 seconds             127.0.0.1:8085->3000/tcp
prometheus   Up 34 seconds             127.0.0.1:9090->9090/tcp
```

‚úÖ **Tous les services de supervision sont op√©rationnels.**

---

### Configuration Post-Installation

#### Prometheus - V√©rification des Targets

1. **Acc√©der √† Prometheus**
   ```
   URL: http://localhost:9090
   ```

2. **V√©rifier les targets (CRITIQUE)**
   - Menu: **Status > Targets**
   - V√©rifier que les 2 jobs sont pr√©sents :
     - `prometheus` : √âtat **UP** (vert)
     - `cadvisor` : √âtat **UP** (vert)

3. **Explorer les m√©triques**
   - Barre de recherche : taper `container_`
   - Auto-compl√©tion montre toutes les m√©triques cAdvisor disponibles
   - Exemples :
     - `container_cpu_usage_seconds_total`
     - `container_memory_usage_bytes`
     - `container_network_receive_bytes_total`

---

#### Grafana - Configuration Initiale

1. **Acc√©der √† Grafana**
   ```
   URL: http://localhost:8085
   ```

2. **Premi√®re connexion**
   - Utilisateur par d√©faut: `admin`
   - Mot de passe par d√©faut: `admin`
   - **‚ö†Ô∏è Grafana force le changement de mot de passe imm√©diatement**

3. **Ajouter Prometheus comme Source de Donn√©es**
   - Ic√¥ne engrenage (menu gauche) > "Data Sources"
   - Cliquer "Add data source"
   - S√©lectionner "Prometheus"
   - **Prometheus server URL:** `http://prometheus:9090`
   - Cliquer "Save & test"
   - Message de succ√®s : "Data source is working" (vert)

4. **Importer un Tableau de Bord Communautaire**
   - Ic√¥ne 4 carr√©s (menu gauche) > "Dashboards"
   - Cliquer "Import"
   - **Import via grafana.com:** Entrer l'ID `13981`
   - Cliquer "Load"
   - S√©lectionner la source de donn√©es Prometheus cr√©√©e
   - Cliquer "Import"

**Dashboard 13981 :** Excellent tableau de bord communautaire pour cAdvisor
- Visualisation CPU, RAM, R√©seau, I/O disque
- Vue par conteneur
- Temps r√©el

---

### R√©capitulatif des Ports (Mise √† jour)

| Service          | Port Local      | URL                            |
|------------------|-----------------|--------------------------------|
| phpLDAPadmin     | 8080            | http://localhost:8080          |
| Zammad           | 8081            | http://localhost:8081          |
| Zammad Rails     | 8082            | http://localhost:8082          |
| OCS Inventory    | 8083            | http://localhost:8083/ocsreports |
| Wiki.js          | 8084            | http://localhost:8084          |
| **Grafana**      | **8085**        | **http://localhost:8085**      |
| **Prometheus**   | **9090**        | **http://localhost:9090**      |
| Portainer (HTTP) | 9000            | http://localhost:9000          |
| Portainer (HTTPS)| 9443            | https://localhost:9443         |

**Note:** cAdvisor n'expose pas de port sur l'h√¥te (port 8080 interne uniquement, accessible par Prometheus)

---

### M√©triques Disponibles

#### Exemples de M√©triques cAdvisor

**Conteneurs :**
- `container_last_seen` : Dernier scrape r√©ussi
- `container_start_time_seconds` : Timestamp de d√©marrage

**CPU :**
- `container_cpu_usage_seconds_total` : Utilisation CPU cumul√©e
- `container_cpu_system_seconds_total` : Temps CPU syst√®me

**M√©moire :**
- `container_memory_usage_bytes` : Utilisation m√©moire actuelle
- `container_memory_max_usage_bytes` : Pic d'utilisation
- `container_memory_working_set_bytes` : Working set (m√©moire active)

**R√©seau :**
- `container_network_receive_bytes_total` : Octets re√ßus
- `container_network_transmit_bytes_total` : Octets envoy√©s
- `container_network_receive_errors_total` : Erreurs de r√©ception

**Disque :**
- `container_fs_usage_bytes` : Utilisation disque
- `container_fs_limit_bytes` : Limite disque

---

## Phase 7 : D√©ploiement de Traefik - Reverse Proxy et HTTPS Automatique

**Date :** 2025-10-22
**Objectif :** D√©ployer Traefik comme reverse proxy pour unifier l'acc√®s √† tous les services sous des sous-domaines propres avec HTTPS automatique via Let's Encrypt

### Th√©orie et Concepts

#### 1. Le Probl√®me : Chaos des Ports
Avant Traefik, l'acc√®s aux services n√©cessitait de m√©moriser de nombreux ports :
- Zammad : `http://localhost:8081`
- OCS Inventory : `http://localhost:8083`
- Wiki.js : `http://localhost:8084`
- Grafana : `http://localhost:8085`
- Prometheus : `http://localhost:9090`
- Portainer : `http://localhost:9443`
- phpLDAPadmin : `http://localhost:8080`

**Inconv√©nients :**
- Non professionnel et difficile √† m√©moriser
- Tout en HTTP non s√©curis√©
- Multiples ports expos√©s = surface d'attaque augment√©e

#### 2. Solution : Reverse Proxy Traefik
**Traefik** agit comme un "chef d'orchestre" du trafic r√©seau :
- **Point d'entr√©e unique** : Seuls les ports 80 (HTTP) et 443 (HTTPS) sont expos√©s
- **Routage intelligent** : Traefik lit le nom de domaine demand√© et dirige vers le bon service
- **Auto-d√©couverte** : D√©tection automatique des conteneurs Docker via labels
- **HTTPS automatique** : Gestion compl√®te des certificats Let's Encrypt

**Flux de requ√™te :**
```
Utilisateur ‚Üí https://zammad.domain.com
    ‚Üì
Traefik (port 443) ‚Üí Lit le domaine "zammad.domain.com"
    ‚Üì
Traefik consulte ses labels Docker
    ‚Üì
Traefik route vers conteneur zammad-nginx:8080
    ‚Üì
R√©ponse retourne via Traefik ‚Üí Utilisateur
```

#### 3. Avantages de Traefik

**Auto-d√©couverte Docker :**
- Traefik √©coute le socket Docker (`/var/run/docker.sock`)
- D√©tecte automatiquement les nouveaux conteneurs
- Lit les labels Docker pour cr√©er les routes
- Aucune reconfiguration manuelle n√©cessaire

**Let's Encrypt Int√©gr√© :**
- Demande automatiquement un certificat HTTPS
- Prouve le contr√¥le du domaine (HTTP Challenge)
- Installe et renouvelle automatiquement les certificats
- Stockage s√©curis√© dans `acme.json`

**S√©curit√© :**
- Redirection automatique HTTP ‚Üí HTTPS
- Isolation des services (pas de ports expos√©s directement)
- Authentification basique pour le dashboard Traefik
- Filtrage par `exposedByDefault: false`

---

### √âtape 1 : Ajout des Variables d'Environnement

**Fichier `.env` :**
```bash
# Variables ajout√©es pour Traefik
ACME_EMAIL=admin@${DOMAIN}
```

**Explication :**
- `ACME_EMAIL` : Email pour les notifications Let's Encrypt (expiration de certificats)
- Pour un vrai domaine, remplacer par une vraie adresse email valide

---

### √âtape 2 : Cr√©ation de la Configuration Traefik

#### Fichier `config/traefik/traefik.yml`

**Cr√©ation du fichier :**
```bash
# Cr√©er le dossier config/traefik (s'il n'existe pas)
mkdir -p config/traefik

# Cr√©er le fichier de configuration
cat > config/traefik/traefik.yml <<'EOF'
# Configuration statique de Traefik
global:
  checkNewVersion: true
  sendAnonymousUsage: false

# Points d'entr√©e des requ√™tes (HTTP et HTTPS)
entryPoints:
  web:
    address: ":80"
    http:
      redirections:
        entryPoint:
          to: websecure
          scheme: https
  websecure:
    address: ":443"

# Configuration de l'API et du Dashboard Traefik
api:
  dashboard: true
  insecure: false

# Comment Traefik d√©couvre les autres services (via Docker)
providers:
  docker:
    endpoint: "unix:///var/run/docker.sock"
    exposedByDefault: false # Important pour la s√©curit√© !

# Configuration du r√©solveur de certificats Let's Encrypt
certificatesResolvers:
  letsencrypt:
    acme:
      email: ${ACME_EMAIL}
      storage: acme.json
      httpChallenge:
        entryPoint: web
EOF
```

**Explication des sections :**

**EntryPoints :**
- `web` (port 80) : Redirige automatiquement vers HTTPS
- `websecure` (port 443) : Point d'entr√©e HTTPS principal

**API Dashboard :**
- `dashboard: true` : Active le dashboard de Traefik
- `insecure: false` : Le dashboard n√©cessite une route et une authentification

**Providers Docker :**
- `endpoint` : Connexion au socket Docker pour d√©tecter les conteneurs
- `exposedByDefault: false` : Seuls les services avec `traefik.enable=true` sont expos√©s

**CertificatesResolvers :**
- `letsencrypt` : Nom du r√©solveur de certificats
- `httpChallenge` : M√©thode de validation (Let's Encrypt contacte le port 80 pour v√©rifier)
- `storage: acme.json` : Fichier de stockage des certificats

#### Fichier `acme.json`

**Cr√©ation et s√©curisation :**
```bash
# Cr√©er le fichier vide
touch config/traefik/acme.json

# S√©curiser avec permissions 600 (lecture/√©criture propri√©taire uniquement)
chmod 600 config/traefik/acme.json
```

**Note Windows/WSL :**
```powershell
# Sur PowerShell, utiliser wsl pour chmod
wsl chmod 600 /home/ianis/TicketingOntheFly/config/traefik/acme.json
```

**Importance :** Les permissions 600 sont requises par Traefik pour raisons de s√©curit√© (fichier contient les cl√©s priv√©es des certificats).

---

### √âtape 3 : Modification du docker-compose.yml

#### Ajout du Service Traefik

**Commande :**
```yaml
# Ajout en d√©but de fichier apr√®s "services:"
traefik:
  image: traefik:v2.10
  container_name: traefik
  restart: unless-stopped
  ports:
    # Seuls ports expos√©s publiquement
    - "80:80"
    - "443:443"
  volumes:
    - /var/run/docker.sock:/var/run/docker.sock:ro
    - ./config/traefik/traefik.yml:/etc/traefik/traefik.yml:ro
    - ./config/traefik/acme.json:/acme.json
  networks:
    - ticketing_network
  labels:
    # Dashboard Traefik
    - "traefik.enable=true"
    - "traefik.http.routers.traefik.rule=Host(`traefik.${DOMAIN}`)"
    - "traefik.http.routers.traefik.entrypoints=websecure"
    - "traefik.http.routers.traefik.tls.certresolver=letsencrypt"
    - "traefik.http.routers.traefik.service=api@internal"
    # Authentification basique
    - "traefik.http.routers.traefik.middlewares=auth"
    - "traefik.http.middlewares.auth.basicauth.users=admin:$$apr1$$LM59Ds56$$5jf2lGXS1Q3tjdxGJw31i."
```

**G√©n√©ration du mot de passe hash√© :**
```bash
# Utiliser htpasswd via Docker
docker run --rm httpd:2.4-alpine htpasswd -nb admin TicketingAdmin2025

# R√©sultat : admin:$apr1$LM59Ds56$5jf2lGXS1Q3tjdxGJw31i.
# IMPORTANT : Dans docker-compose.yml, doubler les $ : $$
```

**Explication des volumes :**
- `/var/run/docker.sock` : Acc√®s Docker en lecture seule (`:ro`)
- `traefik.yml` : Configuration statique en lecture seule
- `acme.json` : Fichier de certificats en lecture/√©criture

#### Suppression des Ports et Ajout des Labels

**Pour chaque service √† exposer, effectuer 2 modifications :**

**1. Supprimer la section `ports:`**
```yaml
# AVANT
ports:
  - "127.0.0.1:8081:8080"

# APR√àS : Section ports compl√®tement supprim√©e
```

**2. Ajouter les labels Traefik**
```yaml
labels:
  - "traefik.enable=true"
  - "traefik.http.routers.SERVICE.rule=Host(`SERVICE.${DOMAIN}`)"
  - "traefik.http.routers.SERVICE.entrypoints=websecure"
  - "traefik.http.routers.SERVICE.tls.certresolver=letsencrypt"
  - "traefik.http.services.SERVICE.loadbalancer.server.port=PORT_INTERNE"
```

**Services modifi√©s :**

| Service | Sous-domaine | Port interne | Labels ajout√©s |
|---------|--------------|--------------|----------------|
| zammad-nginx | `zammad.${DOMAIN}` | 8080 | ‚úÖ |
| ocs-server | `ocs.${DOMAIN}` | 80 | ‚úÖ |
| wikijs | `wiki.${DOMAIN}` | 3000 | ‚úÖ |
| phpldapadmin | `ldap.${DOMAIN}` | 80 | ‚úÖ |
| prometheus | `prometheus.${DOMAIN}` | 9090 | ‚úÖ |
| grafana | `grafana.${DOMAIN}` | 3000 | ‚úÖ |
| portainer | `portainer.${DOMAIN}` | 9000 | ‚úÖ |

**Services NON expos√©s (pas de labels) :**
- `zammad-db`, `ocs-db` : Bases de donn√©es (s√©curit√©)
- `zammad-elasticsearch`, `zammad-redis` : Services internes
- `openldap` : Annuaire (acc√®s via phpLDAPadmin)
- `cadvisor` : M√©triques (acc√®s via Prometheus)

---

### √âtape 4 : D√©ploiement de Traefik

**Commande de d√©ploiement :**
```bash
# Recr√©er tous les services avec la nouvelle configuration
docker-compose up -d
```

**R√©sultat observ√© :**
```
[+] Running 5/5
 ‚úî traefik Pulled                                   9.1s
[+] Running 18/18
 ‚úî Container traefik               Started          2.3s
 ‚úî Container portainer             Recreated        1.0s
 ‚úî Container prometheus            Recreated        1.2s
 ‚úî Container grafana               Started          2.1s
 ‚úî Container wikijs                Started          2.1s
 ‚úî Container ocs-server            Started          2.2s
 ‚úî Container phpldapadmin          Started          2.0s
 ‚úî Container zammad-nginx          Started          1.4s
 ... (tous les autres services)
```

**V√©rification de l'√©tat :**
```bash
docker-compose ps | Select-String -Pattern "traefik|Up"
```

**R√©sultat :**
```
traefik    traefik:v2.10    Up 10 seconds    0.0.0.0:80->80/tcp, 0.0.0.0:443->443/tcp
grafana    grafana/...      Up 10 seconds    3000/tcp
prometheus prom/...         Up 10 seconds    9090/tcp
wikijs     requarks/...     Up 10 seconds    3000/tcp
...
```

**Observations :**
- Seul Traefik expose les ports 80 et 443
- Tous les autres services n'ont plus de ports expos√©s sur l'h√¥te
- Les ports internes (3000, 8080, etc.) restent accessibles via Docker network

---

### √âtape 5 : V√©rification des Logs Traefik

**Commande :**
```bash
docker logs traefik --tail 30
```

**Logs observ√©s :**
```
time="2025-10-22T11:55:42Z" level=info msg="Configuration loaded from file: /etc/traefik/traefik.yml"
time="2025-10-22T11:55:45Z" level=error msg="Unable to obtain ACME certificate for domains \"portainer.localhost\": 
  cannot get ACME client acme: error: 400 :: urn:ietf:params:acme:error:invalidContact :: 
  Error validating contact(s) :: unable to parse email address" 
  providerName=letsencrypt.acme routerName=portainer@docker
...
```

**Analyse :**
- ‚úÖ Configuration charg√©e avec succ√®s
- ‚úÖ Traefik d√©tecte tous les services (portainer, traefik, prometheus, wiki, grafana, ocs, ldap, zammad)
- ‚ö†Ô∏è Erreurs ACME attendues car :
  - `DOMAIN=localhost` ‚Üí email devient `admin@localhost` (invalide)
  - Let's Encrypt ne peut pas √©mettre de certificats pour "localhost"

**Note :** Ces erreurs dispara√Ætront en production avec un vrai domaine et un vrai email.

---

### √âtape 6 : Acc√®s aux Services

#### En D√©veloppement Local (DOMAIN=localhost)

**Probl√®me :** Les sous-domaines `*.localhost` ne fonctionnent pas par d√©faut dans les navigateurs.

**Solutions temporaires :**

**Option 1 : Modifier le fichier hosts**
```bash
# Sur Linux/Mac : /etc/hosts
# Sur Windows : C:\Windows\System32\drivers\etc\hosts

127.0.0.1 zammad.localhost
127.0.0.1 grafana.localhost
127.0.0.1 wiki.localhost
127.0.0.1 ocs.localhost
127.0.0.1 portainer.localhost
127.0.0.1 prometheus.localhost
127.0.0.1 ldap.localhost
127.0.0.1 traefik.localhost
```

**Option 2 : Utiliser un service DNS local comme dnsmasq**

**Option 3 : Tester avec curl**
```bash
curl -H "Host: zammad.localhost" http://localhost
```

**URLs d'acc√®s (apr√®s configuration hosts) :**
- Dashboard Traefik : `http://traefik.localhost` (admin/TicketingAdmin2025)
- Zammad : `http://zammad.localhost`
- Grafana : `http://grafana.localhost`
- Wiki.js : `http://wiki.localhost`
- OCS Inventory : `http://ocs.localhost`
- Portainer : `http://portainer.localhost`
- Prometheus : `http://prometheus.localhost`
- phpLDAPadmin : `http://ldap.localhost`

**Note :** En local, tout est en HTTP car Let's Encrypt n'√©met pas de certificats pour localhost.

#### En Production (avec un vrai domaine)

**Pr√©requis DNS :** Cr√©er des enregistrements DNS de type A pointant vers l'IP publique du serveur :
```
zammad.mondomaine.com      A    <IP_PUBLIQUE_SERVEUR>
grafana.mondomaine.com     A    <IP_PUBLIQUE_SERVEUR>
wiki.mondomaine.com        A    <IP_PUBLIQUE_SERVEUR>
ocs.mondomaine.com         A    <IP_PUBLIQUE_SERVEUR>
portainer.mondomaine.com   A    <IP_PUBLIQUE_SERVEUR>
prometheus.mondomaine.com  A    <IP_PUBLIQUE_SERVEUR>
ldap.mondomaine.com        A    <IP_PUBLIQUE_SERVEUR>
traefik.mondomaine.com     A    <IP_PUBLIQUE_SERVEUR>
```

**Configuration .env pour production :**
```bash
DOMAIN=mondomaine.com
ACME_EMAIL=admin@mondomaine.com
```

**Processus automatique Let's Encrypt :**
1. Utilisateur acc√®de √† `https://zammad.mondomaine.com`
2. Traefik d√©tecte qu'aucun certificat n'existe
3. Traefik demande un certificat √† Let's Encrypt
4. Let's Encrypt contacte `http://zammad.mondomaine.com/.well-known/acme-challenge/` pour v√©rifier
5. Traefik r√©pond au challenge
6. Let's Encrypt √©met le certificat
7. Traefik installe le certificat et le stocke dans `acme.json`
8. Renouvellement automatique 30 jours avant expiration

**URLs d'acc√®s (production) :**
- `https://zammad.mondomaine.com` ‚úÖ Cadenas vert
- `https://grafana.mondomaine.com` ‚úÖ Cadenas vert
- Toutes les requ√™tes HTTP sont redirig√©es vers HTTPS automatiquement

---

### Configuration des Labels Traefik - Explication D√©taill√©e

**Anatomie d'un label Traefik :**
```yaml
labels:
  # 1. Activer Traefik pour ce conteneur
  - "traefik.enable=true"
  
  # 2. D√©finir la r√®gle de routage (par nom de domaine)
  - "traefik.http.routers.SERVICE_NAME.rule=Host(`sous-domaine.${DOMAIN}`)"
  
  # 3. Sp√©cifier le point d'entr√©e (websecure = HTTPS port 443)
  - "traefik.http.routers.SERVICE_NAME.entrypoints=websecure"
  
  # 4. Configurer le r√©solveur de certificats
  - "traefik.http.routers.SERVICE_NAME.tls.certresolver=letsencrypt"
  
  # 5. Indiquer le port interne du conteneur
  - "traefik.http.services.SERVICE_NAME.loadbalancer.server.port=PORT_INTERNE"
```

**Exemple concret - Grafana :**
```yaml
labels:
  - "traefik.enable=true"
  - "traefik.http.routers.grafana.rule=Host(`grafana.${DOMAIN}`)"
  - "traefik.http.routers.grafana.entrypoints=websecure"
  - "traefik.http.routers.grafana.tls.certresolver=letsencrypt"
  - "traefik.http.services.grafana.loadbalancer.server.port=3000"
```

**Explication :**
- `traefik.enable=true` : Traefik doit g√©rer ce conteneur
- `Host(`grafana.${DOMAIN}`)` : R√©pondre aux requ√™tes pour grafana.localhost (ou grafana.domain.com)
- `entrypoints=websecure` : Utiliser le port 443 (HTTPS)
- `certresolver=letsencrypt` : Obtenir un certificat via Let's Encrypt
- `server.port=3000` : Grafana √©coute sur le port 3000 √† l'int√©rieur du conteneur

---

### Architecture Finale

**Sch√©ma de l'infrastructure :**
```
Internet
   ‚Üì
Ports 80/443 (Traefik)
   ‚Üì
Docker Network: ticketing_network
   ‚îú‚îÄ zammad-nginx:8080       ‚Üí zammad.domain.com
   ‚îú‚îÄ grafana:3000            ‚Üí grafana.domain.com
   ‚îú‚îÄ prometheus:9090         ‚Üí prometheus.domain.com
   ‚îú‚îÄ wikijs:3000             ‚Üí wiki.domain.com
   ‚îú‚îÄ ocs-server:80           ‚Üí ocs.domain.com
   ‚îú‚îÄ portainer:9000          ‚Üí portainer.domain.com
   ‚îú‚îÄ phpldapadmin:80         ‚Üí ldap.domain.com
   ‚îú‚îÄ cadvisor:8080           (interne, via prometheus)
   ‚îú‚îÄ openldap:389            (interne, via phpldapadmin)
   ‚îî‚îÄ databases               (internes, inaccessibles depuis l'ext√©rieur)
```

**S√©curit√© :**
- ‚úÖ Un seul point d'entr√©e (ports 80/443)
- ‚úÖ Redirection automatique HTTP ‚Üí HTTPS
- ‚úÖ Certificats SSL valides
- ‚úÖ Services internes (DB, LDAP, cAdvisor) non expos√©s
- ‚úÖ Dashboard Traefik prot√©g√© par authentification basique
- ‚úÖ Filtrage par `exposedByDefault: false`

---

### Tableau R√©capitulatif des URLs

| Service | Sous-domaine | URL Locale | URL Production |
|---------|--------------|------------|----------------|
| Dashboard Traefik | traefik | http://traefik.localhost | https://traefik.domain.com |
| Zammad | zammad | http://zammad.localhost | https://zammad.domain.com |
| Grafana | grafana | http://grafana.localhost | https://grafana.domain.com |
| Prometheus | prometheus | http://prometheus.localhost | https://prometheus.domain.com |
| Wiki.js | wiki | http://wiki.localhost | https://wiki.domain.com |
| OCS Inventory | ocs | http://ocs.localhost | https://ocs.domain.com |
| Portainer | portainer | http://portainer.localhost | https://portainer.domain.com |
| phpLDAPadmin | ldap | http://ldap.localhost | https://ldap.domain.com |

**Authentification Dashboard Traefik :**
- Utilisateur : `admin`
- Mot de passe : `TicketingAdmin2025`

---

### Prochaines √âtapes

1. **Configuration DNS (Production uniquement)**
   - Acheter un nom de domaine
   - Configurer les enregistrements A dans le panneau DNS
   - Mettre √† jour `.env` avec le vrai domaine et email

2. **S√©curisation Avanc√©e**
   - Ajouter des middlewares pour IP whitelisting
   - Configurer des headers de s√©curit√© (HSTS, CSP)
   - Limiter l'acc√®s √† phpLDAPadmin et Portainer par IP

3. **Monitoring Traefik**
   - Ajouter des m√©triques Traefik dans Prometheus
   - Cr√©er un dashboard Grafana pour Traefik
   - Configurer des alertes sur les erreurs 502/503

4. **Phase 8 : Consolidation**
   - Sauvegardes automatiques
   - Tests de r√©cup√©ration
   - Documentation finale


---



## Phase 8 : Outils de Gestion et DÈveloppement - MailHog et Portainer

**Date :** 2025-10-22
**Objectif :** Finaliser les outils qui facilitent la gestion quotidienne et le dÈbogage : finaliser Portainer accessible via Traefik et ajouter MailHog pour capturer les emails de test

### ThÈorie et Concepts

#### 1. Le ProblËme des Tests Email

Les applications comme Zammad envoient de nombreux emails :
- CrÈation de ticket
- RÈponses aux tickets
- Notifications aux agents
- Alertes systËme

**ProblËmes en dÈveloppement :**
-  Configurer un vrai serveur SMTP est complexe
-  Risque d'envoyer des emails de test ‡ de vrais utilisateurs
-  Les emails peuvent Ítre marquÈs comme spam
-  Difficile de vÈrifier le contenu sans accÈder ‡ une vraie boÓte email

#### 2. Solution : MailHog - Serveur SMTP Factice

**MailHog** est un "piËge ‡ emails" :
- Intercepte tous les emails envoyÈs
- N'envoie **JAMAIS** les emails vers l'extÈrieur
- Affiche les emails capturÈs dans une interface web
- Parfait pour le dÈveloppement et les tests

**Avantages :**
-  Aucun risque de spam accidentel
-  Visualisation immÈdiate du rendu email
-  VÈrification des destinataires et du contenu
-  Test de toute la chaÓne d'envoi sans configuration SMTP complexe

**Fonctionnement :**
\\\
Application (Zammad)  Envoie email au SMTP (mailhog:1025)
    
MailHog capture l'email
    
Administrateur visualise via http://mailhog.localhost:8025
\\\

#### 3. Portainer : Interface de Gestion Visuelle

**Portainer** a ÈtÈ dÈployÈ dËs la Phase 1, maintenant il est complËtement intÈgrÈ :
- Accessible via Traefik : \http://portainer.localhost\
- Gestion visuelle de tous les conteneurs
- Consultation des logs en temps rÈel
- AccËs terminal aux conteneurs
- Gestion des volumes et rÈseaux

---

### …tape 1 : Ajout des Variables d'Environnement SMTP

**Fichier \.env\ modifiÈ :**
\\\ash
# --- Configuration SMTP pour Zammad (vers MailHog) ---
# En production, remplacer par un vrai serveur SMTP
ZAMMAD_SMTP_HOST=mailhog
ZAMMAD_SMTP_PORT=1025
ZAMMAD_SMTP_USER=
ZAMMAD_SMTP_PASSWORD=
ZAMMAD_SMTP_DOMAIN=\
\\\

**Explication des variables :**
- \ZAMMAD_SMTP_HOST=mailhog\ : Nom du service Docker (rÈsolution DNS automatique)
- \ZAMMAD_SMTP_PORT=1025\ : Port SMTP de MailHog (standard : 1025)
- \ZAMMAD_SMTP_USER\ et \ZAMMAD_SMTP_PASSWORD\ : Vides (MailHog ne requiert pas d'authentification)
- \ZAMMAD_SMTP_DOMAIN=\\ : Domaine utilisÈ dans les emails (\From: notifications@localhost\)

**Pour la production :**
```bash
# Exemple avec un vrai serveur SMTP
ZAMMAD_SMTP_HOST=smtp.gmail.com
ZAMMAD_SMTP_PORT=587
ZAMMAD_SMTP_USER=notifications@mondomaine.com
ZAMMAD_SMTP_PASSWORD=MonMotDePasseSecurise
ZAMMAD_SMTP_DOMAIN=mondomaine.com
```

---

### √âtape 2 : Ajout du Service MailHog

**Service ajout√© au `docker-compose.yml` :**
```yaml
# --- Serveur SMTP de test : MailHog ---
mailhog:
  image: mailhog/mailhog:latest
  container_name: mailhog
  restart: unless-stopped
  networks:
    - ticketing_network
  labels:
    - "traefik.enable=true"
    - "traefik.http.routers.mailhog.rule=Host(`mailhog.${DOMAIN}`)"
    - "traefik.http.routers.mailhog.entrypoints=websecure"
    - "traefik.http.routers.mailhog.tls.certresolver=letsencrypt"
    - "traefik.http.services.mailhog.loadbalancer.server.port=8025"
```

**Caract√©ristiques :**
- **Ports internes :**  `1025` (SMTP), `8025` (Interface web)
- **Exposition :** Via Traefik uniquement
- **Stockage :** En m√©moire (pas de volumes)

---

### √âtape 3 : Configuration SMTP dans Zammad

**Variables SMTP ajout√©es √† tous les services Zammad :**
- `zammad-init`
- `zammad-railsserver`
- `zammad-websocket`
- `zammad-scheduler`

**Variables ajout√©es :**
```yaml
environment:
  - SMTP_ADDRESS=${ZAMMAD_SMTP_HOST}
  - SMTP_PORT=${ZAMMAD_SMTP_PORT}
  - SMTP_USER=${ZAMMAD_SMTP_USER}
  - SMTP_PASS=${ZAMMAD_SMTP_PASSWORD}
  - SMTP_DOMAIN=${ZAMMAD_SMTP_DOMAIN}
```

---

### √âtape 4 : D√©ploiement

**Commande :**
```bash
docker-compose up -d
```

**R√©sultat :**
```
[+] Running 1/1
 ‚úî mailhog Pulled                                   2.5s
[+] Running 19/19
 ‚úî Container mailhog               Created          0.1s
 ‚úî Container zammad-init           Started          1.2s
 ‚úî Container zammad-railsserver    Started          1.3s
 ... (services Zammad recr√©√©s avec nouvelles variables)
```

---

### √âtape 5 : V√©rification

**Logs MailHog :**
```bash
docker logs mailhog --tail 20
```

**R√©sultat :**
```
2025/10/22 12:05:01 Using in-memory storage
2025/10/22 12:05:01 [SMTP] Binding to address: 0.0.0.0:1025
2025/10/22 12:05:01 Serving under http://0.0.0.0:8025/
```

‚úÖ MailHog op√©rationnel sur les ports 1025 (SMTP) et 8025 (web)

**D√©tection Traefik :**
```bash
docker logs traefik | Select-String -Pattern "mailhog"
```

‚úÖ Routeur `mailhog@docker` cr√©√© avec r√®gle `Host(mailhog.localhost)`

---

### URLs d'Acc√®s

| Service | URL Locale | URL Production |
|---------|------------|----------------|
| **MailHog** | http://mailhog.localhost | https://mailhog.mondomaine.com |
| **Portainer** | http://portainer.localhost | https://portainer.mondomaine.com |

---

### Test de l'Envoi d'Email

**Proc√©dure :**
1. Acc√©der √† MailHog : `http://mailhog.localhost`
2. Acc√©der √† Zammad : `http://zammad.localhost`
3. Cr√©er un ticket ou ajouter une r√©ponse
4. V√©rifier la r√©ception instantan√©e dans MailHog
5. Examiner l'email : exp√©diteur, destinataire, sujet, contenu HTML

---

### Architecture Mise √† Jour

```
Internet
   ‚Üì
Ports 80/443 (Traefik)
   ‚Üì
Docker Network: ticketing_network
   ‚îú‚îÄ mailhog:8025         ‚Üí mailhog.domain.com (NOUVEAU)
   ‚îÇ    ‚Üë Port SMTP: 1025
   ‚îÇ    ‚Üë Zammad ‚Üí mailhog:1025
   ‚îú‚îÄ portainer:9000       ‚Üí portainer.domain.com
   ‚îú‚îÄ zammad-nginx:8080    ‚Üí zammad.domain.com
   ‚îî‚îÄ ... (autres services)
```

---

### Transition Production

**Remplacer MailHog par un vrai SMTP en production :**

```bash
# .env (production)
ZAMMAD_SMTP_HOST=smtp.sendgrid.net
ZAMMAD_SMTP_PORT=587
ZAMMAD_SMTP_USER=apikey
ZAMMAD_SMTP_PASSWORD=SG.xxxxxxxxxxxx
ZAMMAD_SMTP_DOMAIN=mondomaine.com
```

**D√©sactiver MailHog :**
```yaml
# docker-compose.yml
# mailhog:
#   # Service d√©sactiv√© en production
```

---

### Prochaines √âtapes

1. **Phase 9 : Consolidation Finale**
   - Sauvegardes automatiques
   - Scripts de restauration
   - Tests de r√©cup√©ration
   - Documentation finale
   - Guide de maintenance

---
